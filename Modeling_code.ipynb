{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-D_qgaytQua"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dataset_file = './df'\n",
        "df = pd.read_csv(dataset_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc-TuMXCtXFe"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install rich[jupyter]\n",
        "\n",
        "# Importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import os\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "\n",
        "# rich: for a better display on terminal\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "\n",
        "# define a rich console logger\n",
        "console = Console(record=True)\n",
        "\n",
        "# to display dataframe in ASCII format\n",
        "def display_df(df):\n",
        "    \"\"\"display dataframe in ASCII format\"\"\"\n",
        "\n",
        "    console = Console()\n",
        "    table = Table(\n",
        "        Column(\"source_text\", justify=\"center\"),\n",
        "        Column(\"target_text\", justify=\"center\"),\n",
        "        title=\"Sample Data\",\n",
        "        pad_edge=False,\n",
        "        box=box.ASCII,\n",
        "    )\n",
        "\n",
        "    for i, row in enumerate(df.values.tolist()):\n",
        "        table.add_row(row[0], row[1])\n",
        "\n",
        "    console.print(table)\n",
        "\n",
        "# training logger to log training progress\n",
        "training_logger = Table(\n",
        "    Column(\"Epoch\", justify=\"center\"),\n",
        "    Column(\"Steps\", justify=\"center\"),\n",
        "    Column(\"Loss\", justify=\"center\"),\n",
        "    title=\"Training Status\",\n",
        "    pad_edge=False,\n",
        "    box=box.ASCII,\n",
        ")\n",
        "\n",
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-j8UkYURqmO"
      },
      "outputs": [],
      "source": [
        "# enable GPU\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "config = tf.compat.v1.ConfigProto(\n",
        "    intra_op_parallelism_threads=1,\n",
        "    inter_op_parallelism_threads=1,\n",
        "    allow_soft_placement=True,\n",
        "    device_count={\"GPU\": 0},\n",
        ")\n",
        "sess = tf.compat.v1.Session(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4yhYvVatkVJ"
      },
      "outputs": [],
      "source": [
        "class YourDataSetClass(Dataset):\n",
        "    def __init__(\n",
        "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = target_len\n",
        "        self.target_text = self.data[target_text]\n",
        "        self.source_text = self.data[source_text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_text = str(self.source_text[index])\n",
        "        target_text = str(self.target_text[index])\n",
        "\n",
        "        # 데이터 정제\n",
        "        source_text = \" \".join(source_text.split())\n",
        "        target_text = \" \".join(target_text.split())\n",
        "\n",
        "        #원본 및 요약본 학습 데이터 tokenizer\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text],\n",
        "            max_length=self.source_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text],\n",
        "            max_length=self.summ_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        #input_ids는 모델에 의해 입력으로 사용될 시퀀스를 구축하는 토큰의 숫자 표현인 토큰 인덱스\n",
        "        #attention mask는 시퀀스를 함께 일괄 처리할 때 사용되는 선택적 인수\n",
        "        #source data와 target data각각 input_ids, attention_mark를 변수로 만들어줌. \n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        source_mask = source[\"attention_mask\"].squeeze()\n",
        "        target_ids = target[\"input_ids\"].squeeze()\n",
        "        target_mask = target[\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlj7nAcGtknE"
      },
      "outputs": [],
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    for _, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if _ % 10 == 0:\n",
        "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "            console.print(training_logger)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9s9GgBhtzcv"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "\n",
        "  model_path = ''\n",
        "  model = torch.load(model_path + 'model.pt')\n",
        "  model.eval() \n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=150, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=2.5, \n",
        "              length_penalty=1.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "          if _%10==0:\n",
        "              console.print(f'Completed {_}')\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "  return predictions, actuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFq7wYApt99F"
      },
      "outputs": [],
      "source": [
        "def T5Trainer(\n",
        "    dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\"\n",
        "):\n",
        "\n",
        "    model_path = '축적해서 학습한 model.pt의 경로'\n",
        "    token_path = './ETRI_ET5/'\n",
        "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
        "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained(token_path)\n",
        "\n",
        "    model = torch.load(model_path + 'model.pt')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # raw dataset을 importing\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "    display_df(dataframe.head(2))\n",
        "\n",
        "    # Dataset 및 Dataloader 만들고 train, val split\n",
        "    train_size = 0.8\n",
        "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    # Train, Test datasets 만들기\n",
        "    training_set = YourDataSetClass(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = YourDataSetClass(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # testing과 validation을 위한 dataloader 생성\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
        "    )\n",
        "    #Train\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "    #학습한 model 및 tokenizer 저장\n",
        "    path = os.path.join(output_dir, \"model_files\")\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "    # Validation\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
        "        \n",
        "    #요약 예시를 csv로 저장\n",
        "    os.path.join(output_dir,'predictions.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nTXI8jluA7E"
      },
      "outputs": [],
      "source": [
        "model_path = '축적해서 학습한 model.pt의 경로'\n",
        "model = torch.load(model_path + 'model.pt')\n",
        "\n",
        "#파라미터 조정\n",
        "model_params = {\n",
        "    \"MODEL\": model, \n",
        "    \"TRAIN_BATCH_SIZE\": 3,  # training batch size\n",
        "    \"VALID_BATCH_SIZE\": 3,  # validation batch size\n",
        "    \"TRAIN_EPOCHS\": 3,  # number of training epochs\n",
        "    \"VAL_EPOCHS\": 3,  # number of validation epochs\n",
        "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": 600,  # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": 150,  # max length of target text\n",
        "    \"SEED\": 42}  #set seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-k4dxMGuDMW"
      },
      "outputs": [],
      "source": [
        "df[\"text\"] = \"summarize: \" + df[\"text\"]\n",
        "\n",
        "T5Trainer(\n",
        "    dataframe=df,\n",
        "    source_text=\"text\",\n",
        "    target_text=\"summary\",\n",
        "    model_params=model_params,\n",
        "    output_dir=\"results\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Modeling_code",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}