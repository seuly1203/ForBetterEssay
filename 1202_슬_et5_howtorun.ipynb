{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1202_슬_et5_howtorun.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1ka2fa-IdfwXUI0ZC98jpSO-RFWaIMIE5",
      "authorship_tag": "ABX9TyOMFonXRO3oU4Ki/NjwQQDL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seuly1203/cakd3_Project3/blob/main/1202_%EC%8A%AC_et5_howtorun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLt7ItPqW7Ee"
      },
      "source": [
        "#DATA_DIR = /content/drive/MyDrive/211130_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiBbXRlTV-F6",
        "outputId": "4b8a95db-5e06-4f05-b62b-d0aa44b02a8d"
      },
      "source": [
        "% cd /content/drive/MyDrive/211130_test #데이터 다운받을 디렉토리로 변경하세요!\n",
        "! wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000066/data/ynat-v1.1.tar.gz\n",
        "! tar xvzf ynat-v1.1.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/211130_test\n",
            "--2021-11-30 10:19:35--  https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000066/data/ynat-v1.1.tar.gz\n",
            "Resolving aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)... 52.92.145.113\n",
            "Connecting to aistages-prod-server-public.s3.amazonaws.com (aistages-prod-server-public.s3.amazonaws.com)|52.92.145.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4932512 (4.7M) [application/x-gzip]\n",
            "Saving to: ‘ynat-v1.1.tar.gz’\n",
            "\n",
            "ynat-v1.1.tar.gz    100%[===================>]   4.70M  7.23MB/s    in 0.7s    \n",
            "\n",
            "2021-11-30 10:19:36 (7.23 MB/s) - ‘ynat-v1.1.tar.gz’ saved [4932512/4932512]\n",
            "\n",
            "ynat-v1.1/\n",
            "ynat-v1.1/ynat-v1.1_train.json\n",
            "ynat-v1.1/ynat-v1.1_dev.json\n",
            "ynat-v1.1/ynat-v1.1_dev_sample_10.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4GEs1OLW6XG",
        "outputId": "9b44b360-9063-4445-c4d5-d03327a01b41"
      },
      "source": [
        "% cd ynat-v1.1 # 위에서 다운받은 ynat 파일 바로가기 파일을 만드는 과정입니다\n",
        "! ln -s ynat-v1.1_train.json train.json\n",
        "! ln -s ynat-v1.1_dev.json val.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/211130_test/ynat-v1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_cxifLAXbre"
      },
      "source": [
        "# 3. 가상환경 설치 (권장)\n",
        "\n",
        "#$ conda create -n aiopen_tranformers_4.3.2 python=3.7.10\n",
        "#$ conda activate aiopen_tranformers_4.3.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mPOZsINXfEe",
        "outputId": "a093e7ca-d5c6-4c34-ab83-fa5e3a8f5dde"
      },
      "source": [
        "# 버전 맞추기\n",
        "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 42.9 MB/s eta 0:00:27tcmalloc: large alloc 1147494400 bytes == 0x559d0797c000 @  0x7f8d6251d615 0x559cceb3f4cc 0x559ccec1f47a 0x559cceb422ed 0x559ccec33e1d 0x559ccebb5e99 0x559ccebb09ee 0x559cceb43bda 0x559ccebb5d00 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccec34c66 0x559ccebb1daf 0x559ccec34c66 0x559ccebb1daf 0x559ccec34c66 0x559ccebb1daf 0x559cceb44039 0x559cceb87409 0x559cceb42c52 0x559ccebb5c25 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1915 0x559cceb43afa 0x559ccebb1c0d 0x559ccebb09ee\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 43.9 MB/s eta 0:00:22tcmalloc: large alloc 1434370048 bytes == 0x559d4bfd2000 @  0x7f8d6251d615 0x559cceb3f4cc 0x559ccec1f47a 0x559cceb422ed 0x559ccec33e1d 0x559ccebb5e99 0x559ccebb09ee 0x559cceb43bda 0x559ccebb5d00 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccec34c66 0x559ccebb1daf 0x559ccec34c66 0x559ccebb1daf 0x559ccec34c66 0x559ccebb1daf 0x559cceb44039 0x559cceb87409 0x559cceb42c52 0x559ccebb5c25 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1915 0x559cceb43afa 0x559ccebb1c0d 0x559ccebb09ee\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 40.0 MB/s eta 0:00:17tcmalloc: large alloc 1792966656 bytes == 0x559cd0e04000 @  0x7f8d6251d615 0x559cceb3f4cc 0x559ccec1f47a 0x559cceb422ed 0x559ccec33e1d 0x559ccebb5e99 0x559ccebb09ee 0x559cceb43bda 0x559ccebb5d00 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccec34c66 0x559ccebb1daf 0x559ccec34c66 0x559ccebb1daf 0x559ccec34c66 0x559ccebb1daf 0x559cceb44039 0x559cceb87409 0x559cceb42c52 0x559ccebb5c25 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1915 0x559cceb43afa 0x559ccebb1c0d 0x559ccebb09ee\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.3 MB/s eta 0:03:50tcmalloc: large alloc 2241208320 bytes == 0x559d3bbec000 @  0x7f8d6251d615 0x559cceb3f4cc 0x559ccec1f47a 0x559cceb422ed 0x559ccec33e1d 0x559ccebb5e99 0x559ccebb09ee 0x559cceb43bda 0x559ccebb5d00 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccec34c66 0x559ccebb1daf 0x559ccec34c66 0x559ccebb1daf 0x559ccec34c66 0x559ccebb1daf 0x559cceb44039 0x559cceb87409 0x559cceb42c52 0x559ccebb5c25 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1915 0x559cceb43afa 0x559ccebb1c0d 0x559ccebb09ee\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1982177280 bytes == 0x559dc154e000 @  0x7f8d6251c1e7 0x559cceb75067 0x559cceb3f4cc 0x559ccec1f47a 0x559cceb422ed 0x559ccec33e1d 0x559ccebb5e99 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1c0d 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1c0d 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1c0d 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1c0d 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1c0d 0x559cceb43afa 0x559ccebb1c0d 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccebb09ee\n",
            "tcmalloc: large alloc 2477727744 bytes == 0x559e377a8000 @  0x7f8d6251d615 0x559cceb3f4cc 0x559ccec1f47a 0x559cceb422ed 0x559ccec33e1d 0x559ccebb5e99 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1c0d 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1c0d 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1c0d 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1c0d 0x559ccebb09ee 0x559cceb43bda 0x559ccebb1c0d 0x559cceb43afa 0x559ccebb1c0d 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccebb09ee 0x559cceb43bda 0x559ccebb2737 0x559ccebb09ee 0x559cceb44271\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 2.3 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.6 MB 163 kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.8.1\n",
            "  Downloading torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (3.10.0.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0+cu111\n",
            "    Uninstalling torchaudio-0.10.0+cu111:\n",
            "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.1+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1+cu111 torchaudio-0.8.1 torchvision-0.9.1+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-aU4hw6Xlj1",
        "outputId": "1cafe8ae-2063-435f-e390-0be08472fcf6"
      },
      "source": [
        "# 트랜스포머 버전 맞추기\n",
        "!pip install transformers==4.3.2 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.3.2\n",
            "  Downloading transformers-4.3.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.2) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.2) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.2) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (1.1.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VtZ7gFCoXsum",
        "outputId": "16a281cf-0a39-4a1f-efa8-f6cd09302d3e"
      },
      "source": [
        "#BASE_DIR : 여기서는“finetune-t5-ynat-code.tar.gz” 가 위치될 디렉토리 \n",
        "%cd /content/drive/MyDrive/211130_test \n",
        "#!tar xvzf finetune-t5-ynat-code.tar.gz # 압축 푸는 과정인데 저는 이미 풀려있어서 이 과정 생략했어요\n",
        "%cd finetune-t5-ynat-code\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/211130_test\n",
            "/content/drive/MyDrive/211130_test/finetune-t5-ynat-code\n",
            "Requirement already satisfied: absl-py==0.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: altair==4.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (4.1.0)\n",
            "Collecting argon2-cffi==20.1.0\n",
            "  Downloading argon2_cffi-20.1.0-cp35-abi3-manylinux1_x86_64.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.8.1)\n",
            "Collecting async-generator==1.10\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting attrs==20.3.0\n",
            "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
            "Collecting base58==2.1.0\n",
            "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
            "Collecting bleach==3.3.0\n",
            "  Downloading bleach-3.3.0-py2.py3-none-any.whl (283 kB)\n",
            "\u001b[K     |████████████████████████████████| 283 kB 48.0 MB/s \n",
            "\u001b[?25hCollecting blinker==1.4\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 52.3 MB/s \n",
            "\u001b[?25hCollecting cachetools==4.2.1\n",
            "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
            "Collecting certifi==2020.12.5\n",
            "  Downloading certifi-2020.12.5-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 48.4 MB/s \n",
            "\u001b[?25hCollecting cffi==1.14.5\n",
            "  Downloading cffi-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 41.3 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (7.1.2)\n",
            "Collecting conllu==4.4\n",
            "  Downloading conllu-4.4-py2.py3-none-any.whl (15 kB)\n",
            "Collecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting datasets==1.5.0\n",
            "  Downloading datasets-1.5.0-py3-none-any.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting decorator==5.0.6\n",
            "  Downloading decorator-5.0.6-py3-none-any.whl (8.8 kB)\n",
            "Collecting deepspeed==0.3.13\n",
            "  Downloading deepspeed-0.3.13.tar.gz (334 kB)\n",
            "\u001b[K     |████████████████████████████████| 334 kB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (0.7.1)\n",
            "Collecting dill==0.3.3\n",
            "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.7 MB/s \n",
            "\u001b[?25hCollecting elasticsearch==7.12.0\n",
            "  Downloading elasticsearch-7.12.0-py2.py3-none-any.whl (334 kB)\n",
            "\u001b[K     |████████████████████████████████| 334 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 24)) (0.3)\n",
            "Collecting faiss-cpu==1.7.0\n",
            "  Downloading faiss_cpu-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl (8.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1 MB 25.6 MB/s \n",
            "\u001b[?25hCollecting filelock==3.0.12\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting fire==0.4.0\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting fsspec==0.9.0\n",
            "  Downloading fsspec-0.9.0-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting future==0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 40.5 MB/s \n",
            "\u001b[?25hCollecting git-python==1.0.3\n",
            "  Downloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting gitdb==4.0.7\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting gitpython==3.1.14\n",
            "  Downloading GitPython-3.1.14-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 60.9 MB/s \n",
            "\u001b[?25hCollecting google-auth==1.28.0\n",
            "  Downloading google_auth-1.28.0-py2.py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib==0.4.4\n",
            "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: googleapis-common-protos==1.53.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 35)) (1.53.0)\n",
            "Collecting grpcio==1.37.0\n",
            "  Downloading grpcio-1.37.0-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 45.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 38)) (2.10)\n",
            "Collecting importlib-metadata==3.10.0\n",
            "  Downloading importlib_metadata-3.10.0-py3-none-any.whl (14 kB)\n",
            "Collecting importlib-resources==5.1.2\n",
            "  Downloading importlib_resources-5.1.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: iniconfig==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 41)) (1.1.1)\n",
            "Collecting ipykernel==5.5.3\n",
            "  Downloading ipykernel-5.5.3-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 70.0 MB/s \n",
            "\u001b[?25hCollecting ipython==7.22.0\n",
            "  Downloading ipython-7.22.0-py3-none-any.whl (785 kB)\n",
            "\u001b[K     |████████████████████████████████| 785 kB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 44)) (0.2.0)\n",
            "Collecting ipywidgets==7.6.3\n",
            "  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 64.6 MB/s \n",
            "\u001b[?25hCollecting jedi==0.18.0\n",
            "  Downloading jedi-0.18.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2==2.11.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 47)) (2.11.3)\n",
            "Collecting joblib==1.0.1\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 42.0 MB/s \n",
            "\u001b[?25hCollecting jsonlines==2.0.0\n",
            "  Downloading jsonlines-2.0.0-py3-none-any.whl (6.3 kB)\n",
            "Collecting jsonschema==3.2.0\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting jupyter-client==6.1.12\n",
            "  Downloading jupyter_client-6.1.12-py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 64.9 MB/s \n",
            "\u001b[?25hCollecting jupyter-core==4.7.1\n",
            "  Downloading jupyter_core-4.7.1-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyterlab-pygments==0.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 53)) (0.1.2)\n",
            "Collecting jupyterlab-widgets==1.0.0\n",
            "  Downloading jupyterlab_widgets-1.0.0-py3-none-any.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 63.7 MB/s \n",
            "\u001b[?25hCollecting kiwisolver==1.3.1\n",
            "  Downloading kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 55.3 MB/s \n",
            "\u001b[?25hCollecting markdown==3.3.4\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting markupsafe==1.1.1\n",
            "  Downloading MarkupSafe-1.1.1-cp37-cp37m-manylinux2010_x86_64.whl (33 kB)\n",
            "Collecting matplotlib==3.4.1\n",
            "  Downloading matplotlib-3.4.1-cp37-cp37m-manylinux1_x86_64.whl (10.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3 MB 30.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 59)) (0.8.4)\n",
            "Collecting multiprocess==0.70.11.1\n",
            "  Downloading multiprocess-0.70.11.1-py37-none-any.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 63.6 MB/s \n",
            "\u001b[?25hCollecting nbclient==0.5.3\n",
            "  Downloading nbclient-0.5.3-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting nbconvert==6.0.7\n",
            "  Downloading nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
            "\u001b[K     |████████████████████████████████| 552 kB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbformat==5.1.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 63)) (5.1.3)\n",
            "Requirement already satisfied: nest-asyncio==1.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 64)) (1.5.1)\n",
            "Collecting ninja==1.10.0.post2\n",
            "  Downloading ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 65.7 MB/s \n",
            "\u001b[?25hCollecting nltk==3.6.1\n",
            "  Downloading nltk-3.6.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 36.5 MB/s \n",
            "\u001b[?25hCollecting notebook==6.3.0\n",
            "  Downloading notebook-6.3.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 32.6 MB/s \n",
            "\u001b[?25hCollecting numpy==1.20.2\n",
            "  Downloading numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 36.2 MB/s \n",
            "\u001b[?25hCollecting oauthlib==3.1.0\n",
            "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 64.0 MB/s \n",
            "\u001b[?25hCollecting packaging==20.9\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting pandas==1.2.3\n",
            "  Downloading pandas-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 53.8 MB/s \n",
            "\u001b[?25hCollecting pandocfilters==1.4.3\n",
            "  Downloading pandocfilters-1.4.3.tar.gz (16 kB)\n",
            "Requirement already satisfied: parso==0.8.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 73)) (0.8.2)\n",
            "Requirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 74)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 75)) (0.7.5)\n",
            "Collecting pillow==8.2.0\n",
            "  Downloading Pillow-8.2.0-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 45.5 MB/s \n",
            "\u001b[?25hCollecting pip==21.0.1\n",
            "  Downloading pip-21.0.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 39.1 MB/s \n",
            "\u001b[?25hCollecting pluggy==0.13.1\n",
            "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting prometheus-client==0.10.0\n",
            "  Downloading prometheus_client-0.10.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise==2.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 81)) (2.3)\n",
            "Collecting prompt-toolkit==3.0.18\n",
            "  Downloading prompt_toolkit-3.0.18-py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 53.3 MB/s \n",
            "\u001b[?25hCollecting protobuf==3.15.7\n",
            "  Downloading protobuf-3.15.7-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 40.0 MB/s \n",
            "\u001b[?25hCollecting psutil==5.8.0\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 85)) (0.7.0)\n",
            "Collecting py==1.10.0\n",
            "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow==3.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 87)) (3.0.0)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 88)) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 89)) (0.2.8)\n",
            "Collecting pycparser==2.2\n",
            "  Downloading pycparser-2.02.zip (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 53.5 MB/s \n",
            "\u001b[?25hCollecting pydeck==0.6.1\n",
            "  Downloading pydeck-0.6.1-py2.py3-none-any.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 35.7 MB/s \n",
            "\u001b[?25hCollecting pygments==2.8.1\n",
            "  Downloading Pygments-2.8.1-py3-none-any.whl (983 kB)\n",
            "\u001b[K     |████████████████████████████████| 983 kB 23.2 MB/s \n",
            "\u001b[?25hCollecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting pyrsistent==0.17.3\n",
            "  Downloading pyrsistent-0.17.3.tar.gz (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 63.4 MB/s \n",
            "\u001b[?25hCollecting pytest==6.2.3\n",
            "  Downloading pytest-6.2.3-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.8.1\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 66.5 MB/s \n",
            "\u001b[?25hCollecting pytz==2021.1\n",
            "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 50.3 MB/s \n",
            "\u001b[?25hCollecting pyzmq==22.0.3\n",
            "  Downloading pyzmq-22.0.3-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 34.7 MB/s \n",
            "\u001b[?25hCollecting regex==2021.4.4\n",
            "  Downloading regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720 kB)\n",
            "\u001b[K     |████████████████████████████████| 720 kB 41.4 MB/s \n",
            "\u001b[?25hCollecting requests==2.25.1\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 101)) (1.3.0)\n",
            "Collecting rouge-score==0.0.4\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Collecting rsa==4.7.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting sacrebleu==1.5.1\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.44\n",
            "  Downloading sacremoses-0.0.44.tar.gz (862 kB)\n",
            "\u001b[K     |████████████████████████████████| 862 kB 49.1 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.1\n",
            "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 51.7 MB/s \n",
            "\u001b[?25hCollecting scipy==1.6.2\n",
            "  Downloading scipy-1.6.2-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 62.8 MB/s \n",
            "\u001b[?25hCollecting send2trash==1.5.0\n",
            "  Downloading Send2Trash-1.5.0-py3-none-any.whl (12 kB)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 38.9 MB/s \n",
            "\u001b[?25hCollecting seqeval==1.2.2\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting setuptools==52.0.0\n",
            "  Downloading setuptools-52.0.0-py3-none-any.whl (784 kB)\n",
            "\u001b[K     |████████████████████████████████| 784 kB 46.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 112)) (1.15.0)\n",
            "Collecting smmap==4.0.0\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting streamlit==0.79.0\n",
            "  Downloading streamlit-0.79.0-py2.py3-none-any.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 29.7 MB/s \n",
            "\u001b[?25hCollecting tensorboard==2.4.1\n",
            "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 13.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 116)) (1.8.0)\n",
            "Collecting tensorboardx==1.8\n",
            "  Downloading tensorboardX-1.8-py2.py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 64.4 MB/s \n",
            "\u001b[?25hCollecting tensorflow-datasets==4.2.0\n",
            "  Downloading tensorflow_datasets-4.2.0-py3-none-any.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting tensorflow-metadata==0.29.0\n",
            "  Downloading tensorflow_metadata-0.29.0-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 120)) (1.1.0)\n",
            "Collecting terminado==0.9.4\n",
            "  Downloading terminado-0.9.4-py3-none-any.whl (14 kB)\n",
            "Collecting testpath==0.4.4\n",
            "  Downloading testpath-0.4.4-py2.py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 45.0 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl==2.1.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting tokenizers==0.10.2\n",
            "  Downloading tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 125)) (0.10.2)\n",
            "Collecting toolz==0.11.1\n",
            "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.8.1+cu111 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 127)) (1.8.1+cu111)\n",
            "Requirement already satisfied: torchaudio==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 128)) (0.8.1)\n",
            "Requirement already satisfied: torchvision==0.9.1+cu111 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 129)) (0.9.1+cu111)\n",
            "Collecting tornado==6.1\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     |████████████████████████████████| 428 kB 58.7 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.49.0\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting traitlets==5.0.5\n",
            "  Downloading traitlets-5.0.5-py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers==4.3.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 133)) (4.3.2)\n",
            "Collecting typing-extensions==3.7.4.3\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting tzlocal==2.1\n",
            "  Downloading tzlocal-2.1-py2.py3-none-any.whl (16 kB)\n",
            "Collecting urllib3==1.26.4\n",
            "  Downloading urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 54.6 MB/s \n",
            "\u001b[?25hCollecting validators==0.18.2\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Collecting watchdog==2.0.2\n",
            "  Downloading watchdog-2.0.2-py3-none-manylinux2014_x86_64.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth==0.2.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 139)) (0.2.5)\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 140)) (0.5.1)\n",
            "Requirement already satisfied: werkzeug==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 141)) (1.0.1)\n",
            "Collecting wheel==0.36.2\n",
            "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
            "Collecting widgetsnbextension==3.5.1\n",
            "  Downloading widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 56.5 MB/s \n",
            "\u001b[?25hCollecting xxhash==2.0.0\n",
            "  Downloading xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 49.9 MB/s \n",
            "\u001b[?25hCollecting zipp==3.4.1\n",
            "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
            "Collecting ply\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: blinker, deepspeed, fire, future, pandocfilters, pycparser, pyrsistent, sacremoses, seqeval\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=3be13f96b7273e06b1a6f6a9f0e2d724cc767d3e57747f57f714e110ca0ce3cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.3.13-py3-none-any.whl size=328698 sha256=be7b19370c5f98562b6b560f9f86cd5ecb92c1c128356fd8db7ce2d507c3ef24\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/85/69/43027107b6053f8a15284899ee04513de4080dbcef77240c21\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=dd4bd00227cc893a9b0005e351300ae3a86d317a13ab65f2cbd6afc345af55e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=203f950dcf39d9a3082c998be42ea0549dc3ee0cd0738ee65813412272140ef4\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for pandocfilters (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandocfilters: filename=pandocfilters-1.4.3-py3-none-any.whl size=8006 sha256=d52979fce33b7820132c8f87b23e8b1fbc924803ae0c03069cc2033fdeee7606\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/81/34/545dc2fbf0e9137811e901108d37fc04650e81d48f97078000\n",
            "  Building wheel for pycparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycparser: filename=pycparser-2.2-py3-none-any.whl size=25145 sha256=ebd436e2f59686d8a78600b696f2591e1fa97f212cb2c99c0b3a9e42a2eb99e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c2/e9/e2c8e4cc0a4737e4c3c6090e66e793af1f6db10d5b9384ea31\n",
            "  Building wheel for pyrsistent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrsistent: filename=pyrsistent-0.17.3-cp37-cp37m-linux_x86_64.whl size=98092 sha256=6b27e7e15953a35d1483091879b7f88eac9976dd8386e1101f23535a8d941962\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/52/bf/71258a1d7b3c8cbe1ee53f9314c6f65f20385481eaee573cc5\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-py3-none-any.whl size=886076 sha256=7dfa22d41381bbda612961d0cdc335c6f60bf58e3f0e1160ab92a4562a9d4abb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/92/02/dad900eead4b4a0025d513fa79992095071af492c6188bd589\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=2ee69182400af547afcbbf817fd71ca9b2f72064a021a4a5482cd1a088f205d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built blinker deepspeed fire future pandocfilters pycparser pyrsistent sacremoses seqeval\n",
            "Installing collected packages: zipp, typing-extensions, traitlets, setuptools, pyrsistent, importlib-metadata, attrs, tornado, pyzmq, python-dateutil, pyparsing, ply, jupyter-core, jsonschema, pygments, pycparser, prompt-toolkit, packaging, markupsafe, jupyter-client, jedi, decorator, async-generator, testpath, pandocfilters, nbclient, ipython, cffi, bleach, terminado, send2trash, prometheus-client, nbconvert, ipykernel, argon2-cffi, urllib3, notebook, chardet, certifi, widgetsnbextension, smmap, rsa, requests, pytz, protobuf, oauthlib, numpy, jupyterlab-widgets, cachetools, tqdm, toolz, threadpoolctl, scipy, regex, pillow, pandas, joblib, ipywidgets, google-auth, gitdb, filelock, dill, xxhash, wheel, watchdog, validators, tzlocal, tokenizers, tensorflow-metadata, tensorboardx, scikit-learn, sacremoses, pydeck, py, psutil, portalocker, pluggy, nltk, ninja, multiprocess, markdown, kiwisolver, importlib-resources, huggingface-hub, grpcio, google-auth-oauthlib, gitpython, future, fsspec, cycler, blinker, base58, tensorflow-datasets, tensorboard, streamlit, seqeval, sentencepiece, sacrebleu, rouge-score, pytest, pip, matplotlib, jsonlines, git-python, fire, faiss-cpu, elasticsearch, deepspeed, datasets, conllu\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.6.0\n",
            "    Uninstalling zipp-3.6.0:\n",
            "      Successfully uninstalled zipp-3.6.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pyrsistent\n",
            "    Found existing installation: pyrsistent 0.18.0\n",
            "    Uninstalling pyrsistent-0.18.0:\n",
            "      Successfully uninstalled pyrsistent-0.18.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.2\n",
            "    Uninstalling importlib-metadata-4.8.2:\n",
            "      Successfully uninstalled importlib-metadata-4.8.2\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 21.2.0\n",
            "    Uninstalling attrs-21.2.0:\n",
            "      Successfully uninstalled attrs-21.2.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 22.3.0\n",
            "    Uninstalling pyzmq-22.3.0:\n",
            "      Successfully uninstalled pyzmq-22.3.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.6\n",
            "    Uninstalling pyparsing-3.0.6:\n",
            "      Successfully uninstalled pyparsing-3.0.6\n",
            "  Attempting uninstall: jupyter-core\n",
            "    Found existing installation: jupyter-core 4.9.1\n",
            "    Uninstalling jupyter-core-4.9.1:\n",
            "      Successfully uninstalled jupyter-core-4.9.1\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.21\n",
            "    Uninstalling pycparser-2.21:\n",
            "      Successfully uninstalled pycparser-2.21\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "  Attempting uninstall: jedi\n",
            "    Found existing installation: jedi 0.18.1\n",
            "    Uninstalling jedi-0.18.1:\n",
            "      Successfully uninstalled jedi-0.18.1\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: testpath\n",
            "    Found existing installation: testpath 0.5.0\n",
            "    Uninstalling testpath-0.5.0:\n",
            "      Successfully uninstalled testpath-0.5.0\n",
            "  Attempting uninstall: pandocfilters\n",
            "    Found existing installation: pandocfilters 1.5.0\n",
            "    Uninstalling pandocfilters-1.5.0:\n",
            "      Successfully uninstalled pandocfilters-1.5.0\n",
            "  Attempting uninstall: nbclient\n",
            "    Found existing installation: nbclient 0.5.9\n",
            "    Uninstalling nbclient-0.5.9:\n",
            "      Successfully uninstalled nbclient-0.5.9\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.15.0\n",
            "    Uninstalling cffi-1.15.0:\n",
            "      Successfully uninstalled cffi-1.15.0\n",
            "  Attempting uninstall: bleach\n",
            "    Found existing installation: bleach 4.1.0\n",
            "    Uninstalling bleach-4.1.0:\n",
            "      Successfully uninstalled bleach-4.1.0\n",
            "  Attempting uninstall: terminado\n",
            "    Found existing installation: terminado 0.12.1\n",
            "    Uninstalling terminado-0.12.1:\n",
            "      Successfully uninstalled terminado-0.12.1\n",
            "  Attempting uninstall: send2trash\n",
            "    Found existing installation: Send2Trash 1.8.0\n",
            "    Uninstalling Send2Trash-1.8.0:\n",
            "      Successfully uninstalled Send2Trash-1.8.0\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus-client 0.12.0\n",
            "    Uninstalling prometheus-client-0.12.0:\n",
            "      Successfully uninstalled prometheus-client-0.12.0\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 5.6.1\n",
            "    Uninstalling nbconvert-5.6.1:\n",
            "      Successfully uninstalled nbconvert-5.6.1\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Attempting uninstall: argon2-cffi\n",
            "    Found existing installation: argon2-cffi 21.1.0\n",
            "    Uninstalling argon2-cffi-21.1.0:\n",
            "      Successfully uninstalled argon2-cffi-21.1.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: notebook\n",
            "    Found existing installation: notebook 5.3.1\n",
            "    Uninstalling notebook-5.3.1:\n",
            "      Successfully uninstalled notebook-5.3.1\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.5.2\n",
            "    Uninstalling widgetsnbextension-3.5.2:\n",
            "      Successfully uninstalled widgetsnbextension-3.5.2\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.8\n",
            "    Uninstalling rsa-4.8:\n",
            "      Successfully uninstalled rsa-4.8\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: oauthlib\n",
            "    Found existing installation: oauthlib 3.1.1\n",
            "    Uninstalling oauthlib-3.1.1:\n",
            "      Successfully uninstalled oauthlib-3.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: jupyterlab-widgets\n",
            "    Found existing installation: jupyterlab-widgets 1.0.2\n",
            "    Uninstalling jupyterlab-widgets-1.0.2:\n",
            "      Successfully uninstalled jupyterlab-widgets-1.0.2\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.4\n",
            "    Uninstalling cachetools-4.2.4:\n",
            "      Successfully uninstalled cachetools-4.2.4\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.3\n",
            "    Uninstalling tqdm-4.62.3:\n",
            "      Successfully uninstalled tqdm-4.62.3\n",
            "  Attempting uninstall: toolz\n",
            "    Found existing installation: toolz 0.11.2\n",
            "    Uninstalling toolz-0.11.2:\n",
            "      Successfully uninstalled toolz-0.11.2\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.0.0\n",
            "    Uninstalling threadpoolctl-3.0.0:\n",
            "      Successfully uninstalled threadpoolctl-3.0.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.6.5\n",
            "    Uninstalling ipywidgets-7.6.5:\n",
            "      Successfully uninstalled ipywidgets-7.6.5\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 1.35.0\n",
            "    Uninstalling google-auth-1.35.0:\n",
            "      Successfully uninstalled google-auth-1.35.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.4.0\n",
            "    Uninstalling filelock-3.4.0:\n",
            "      Successfully uninstalled filelock-3.4.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.37.0\n",
            "    Uninstalling wheel-0.37.0:\n",
            "      Successfully uninstalled wheel-0.37.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 1.5.1\n",
            "    Uninstalling tzlocal-1.5.1:\n",
            "      Successfully uninstalled tzlocal-1.5.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.10.3\n",
            "    Uninstalling tokenizers-0.10.3:\n",
            "      Successfully uninstalled tokenizers-0.10.3\n",
            "  Attempting uninstall: tensorflow-metadata\n",
            "    Found existing installation: tensorflow-metadata 1.4.0\n",
            "    Uninstalling tensorflow-metadata-1.4.0:\n",
            "      Successfully uninstalled tensorflow-metadata-1.4.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "  Attempting uninstall: sacremoses\n",
            "    Found existing installation: sacremoses 0.0.46\n",
            "    Uninstalling sacremoses-0.0.46:\n",
            "      Successfully uninstalled sacremoses-0.0.46\n",
            "  Attempting uninstall: py\n",
            "    Found existing installation: py 1.11.0\n",
            "    Uninstalling py-1.11.0:\n",
            "      Successfully uninstalled py-1.11.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.12.2\n",
            "    Uninstalling multiprocess-0.70.12.2:\n",
            "      Successfully uninstalled multiprocess-0.70.12.2\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.3.6\n",
            "    Uninstalling Markdown-3.3.6:\n",
            "      Successfully uninstalled Markdown-3.3.6\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.3.2\n",
            "    Uninstalling kiwisolver-1.3.2:\n",
            "      Successfully uninstalled kiwisolver-1.3.2\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 5.4.0\n",
            "    Uninstalling importlib-resources-5.4.0:\n",
            "      Successfully uninstalled importlib-resources-5.4.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.42.0\n",
            "    Uninstalling grpcio-1.42.0:\n",
            "      Successfully uninstalled grpcio-1.42.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.20.2 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.1+cu111 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 2.4.1 which is incompatible.\n",
            "moviepy 0.2.3.5 requires decorator<5.0,>=4.0.2, but you have decorator 5.0.6 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.18 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 5.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.22.0 which is incompatible.\n",
            "google-colab 1.0.0 requires notebook~=5.3.0; python_version >= \"3.0\", but you have notebook 6.3.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.25.1 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed argon2-cffi-20.1.0 async-generator-1.10 attrs-20.3.0 base58-2.1.0 bleach-3.3.0 blinker-1.4 cachetools-4.2.1 certifi-2020.12.5 cffi-1.14.5 chardet-4.0.0 conllu-4.4 cycler-0.10.0 datasets-1.5.0 decorator-5.0.6 deepspeed-0.3.13 dill-0.3.3 elasticsearch-7.12.0 faiss-cpu-1.7.0 filelock-3.0.12 fire-0.4.0 fsspec-0.9.0 future-0.18.2 git-python-1.0.3 gitdb-4.0.7 gitpython-3.1.14 google-auth-1.28.0 google-auth-oauthlib-0.4.4 grpcio-1.37.0 huggingface-hub-0.0.8 importlib-metadata-3.10.0 importlib-resources-5.1.2 ipykernel-5.5.3 ipython-7.22.0 ipywidgets-7.6.3 jedi-0.18.0 joblib-1.0.1 jsonlines-2.0.0 jsonschema-3.2.0 jupyter-client-6.1.12 jupyter-core-4.7.1 jupyterlab-widgets-1.0.0 kiwisolver-1.3.1 markdown-3.3.4 markupsafe-1.1.1 matplotlib-3.4.1 multiprocess-0.70.11.1 nbclient-0.5.3 nbconvert-6.0.7 ninja-1.10.0.post2 nltk-3.6.1 notebook-6.3.0 numpy-1.20.2 oauthlib-3.1.0 packaging-20.9 pandas-1.2.3 pandocfilters-1.4.3 pillow-8.2.0 pip-21.0.1 pluggy-0.13.1 ply-3.11 portalocker-2.0.0 prometheus-client-0.10.0 prompt-toolkit-3.0.18 protobuf-3.15.7 psutil-5.8.0 py-1.10.0 pycparser-2.2 pydeck-0.6.1 pygments-2.8.1 pyparsing-2.4.7 pyrsistent-0.17.3 pytest-6.2.3 python-dateutil-2.8.1 pytz-2021.1 pyzmq-22.0.3 regex-2021.4.4 requests-2.25.1 rouge-score-0.0.4 rsa-4.7.2 sacrebleu-1.5.1 sacremoses-0.0.44 scikit-learn-0.24.1 scipy-1.6.2 send2trash-1.5.0 sentencepiece-0.1.95 seqeval-1.2.2 setuptools-52.0.0 smmap-4.0.0 streamlit-0.79.0 tensorboard-2.4.1 tensorboardx-1.8 tensorflow-datasets-4.2.0 tensorflow-metadata-0.29.0 terminado-0.9.4 testpath-0.4.4 threadpoolctl-2.1.0 tokenizers-0.10.2 toolz-0.11.1 tornado-6.1 tqdm-4.49.0 traitlets-5.0.5 typing-extensions-3.7.4.3 tzlocal-2.1 urllib3-1.26.4 validators-0.18.2 watchdog-2.0.2 wheel-0.36.2 widgetsnbextension-3.5.1 xxhash-2.0.0 zipp-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "PIL",
                  "cffi",
                  "cycler",
                  "dateutil",
                  "decorator",
                  "google",
                  "ipykernel",
                  "ipywidgets",
                  "jupyter_client",
                  "jupyter_core",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pandas",
                  "pkg_resources",
                  "prompt_toolkit",
                  "psutil",
                  "pygments",
                  "pyparsing",
                  "pytz",
                  "tornado",
                  "traitlets",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wvFMOSytZeWM",
        "outputId": "4154c06c-521a-4ebe-9bac-6c8180c11051"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKWyTdwVZfu0",
        "outputId": "60131332-242e-43b5-cb5d-3e9e5dae07b0"
      },
      "source": [
        "%cd /content/drive/MyDrive/211130_test/finetune-t5-ynat-code"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/211130_test/finetune-t5-ynat-code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzx5Y3eOZI5d",
        "outputId": "a716052b-b8c1-4deb-ba7c-74a51736ba5d"
      },
      "source": [
        "!python seq2seq_finetune_t5_ynat.py -h # -help와 같은 기능 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: seq2seq_finetune_t5_ynat.py [-h] --model_name_or_path\n",
            "                                   MODEL_NAME_OR_PATH\n",
            "                                   [--config_name CONFIG_NAME]\n",
            "                                   [--tokenizer_name TOKENIZER_NAME]\n",
            "                                   [--cache_dir CACHE_DIR]\n",
            "                                   [--freeze_encoder [FREEZE_ENCODER]]\n",
            "                                   [--freeze_embeds [FREEZE_EMBEDS]]\n",
            "                                   --data_dir DATA_DIR [--task TASK]\n",
            "                                   [--max_source_length MAX_SOURCE_LENGTH]\n",
            "                                   [--max_target_length MAX_TARGET_LENGTH]\n",
            "                                   [--val_max_target_length VAL_MAX_TARGET_LENGTH]\n",
            "                                   [--test_max_target_length TEST_MAX_TARGET_LENGTH]\n",
            "                                   [--n_train N_TRAIN] [--n_val N_VAL]\n",
            "                                   [--n_test N_TEST] [--src_lang SRC_LANG]\n",
            "                                   [--tgt_lang TGT_LANG]\n",
            "                                   [--eval_beams EVAL_BEAMS]\n",
            "                                   [--no_ignore_pad_token_for_loss]\n",
            "                                   [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]\n",
            "                                   [--output_dir OUTPUT_DIR]\n",
            "                                   [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "                                   [--do_train [DO_TRAIN]]\n",
            "                                   [--do_eval [DO_EVAL]]\n",
            "                                   [--do_predict [DO_PREDICT]]\n",
            "                                   [--evaluation_strategy {no,steps,epoch}]\n",
            "                                   [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "                                   [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                                   [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "                                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                                   [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "                                   [--learning_rate LEARNING_RATE]\n",
            "                                   [--weight_decay WEIGHT_DECAY]\n",
            "                                   [--adam_beta1 ADAM_BETA1]\n",
            "                                   [--adam_beta2 ADAM_BETA2]\n",
            "                                   [--adam_epsilon ADAM_EPSILON]\n",
            "                                   [--max_grad_norm MAX_GRAD_NORM]\n",
            "                                   [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                                   [--max_steps MAX_STEPS]\n",
            "                                   [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
            "                                   [--warmup_steps WARMUP_STEPS]\n",
            "                                   [--logging_dir LOGGING_DIR]\n",
            "                                   [--logging_first_step [LOGGING_FIRST_STEP]]\n",
            "                                   [--logging_steps LOGGING_STEPS]\n",
            "                                   [--save_steps SAVE_STEPS]\n",
            "                                   [--save_total_limit SAVE_TOTAL_LIMIT]\n",
            "                                   [--no_cuda [NO_CUDA]] [--seed SEED]\n",
            "                                   [--fp16 [FP16]]\n",
            "                                   [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                                   [--fp16_backend {auto,amp,apex}]\n",
            "                                   [--local_rank LOCAL_RANK]\n",
            "                                   [--tpu_num_cores TPU_NUM_CORES]\n",
            "                                   [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
            "                                   [--debug [DEBUG]]\n",
            "                                   [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "                                   [--eval_steps EVAL_STEPS]\n",
            "                                   [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                                   [--past_index PAST_INDEX]\n",
            "                                   [--run_name RUN_NAME]\n",
            "                                   [--disable_tqdm DISABLE_TQDM]\n",
            "                                   [--no_remove_unused_columns]\n",
            "                                   [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "                                   [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "                                   [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "                                   [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "                                   [--greater_is_better GREATER_IS_BETTER]\n",
            "                                   [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
            "                                   [--sharded_ddp [SHARDED_DDP]]\n",
            "                                   [--deepspeed DEEPSPEED]\n",
            "                                   [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "                                   [--adafactor [ADAFACTOR]]\n",
            "                                   [--group_by_length [GROUP_BY_LENGTH]]\n",
            "                                   [--report_to REPORT_TO [REPORT_TO ...]]\n",
            "                                   [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "                                   [--no_dataloader_pin_memory]\n",
            "                                   [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "                                   [--label_smoothing LABEL_SMOOTHING]\n",
            "                                   [--sortish_sampler [SORTISH_SAMPLER]]\n",
            "                                   [--predict_with_generate [PREDICT_WITH_GENERATE]]\n",
            "                                   [--encoder_layerdrop ENCODER_LAYERDROP]\n",
            "                                   [--decoder_layerdrop DECODER_LAYERDROP]\n",
            "                                   [--dropout DROPOUT]\n",
            "                                   [--attention_dropout ATTENTION_DROPOUT]\n",
            "                                   [--lr_scheduler LR_SCHEDULER]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        Path to pretrained model or model identifier from\n",
            "                        huggingface.co/models\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pretrained models\n",
            "                        downloaded from huggingface.co\n",
            "  --freeze_encoder [FREEZE_ENCODER]\n",
            "                        Whether tp freeze the encoder.\n",
            "  --freeze_embeds [FREEZE_EMBEDS]\n",
            "                        Whether to freeze the embeddings.\n",
            "  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\n",
            "                        other data files) for the task.\n",
            "  --task TASK           Task name, summarization (or summarization_{dataset}\n",
            "                        for pegasus) or translation\n",
            "  --max_source_length MAX_SOURCE_LENGTH\n",
            "                        The maximum total input sequence length after\n",
            "                        tokenization. Sequences longer than this will be\n",
            "                        truncated, sequences shorter will be padded.\n",
            "  --max_target_length MAX_TARGET_LENGTH\n",
            "                        The maximum total sequence length for target text\n",
            "                        after tokenization. Sequences longer than this will be\n",
            "                        truncated, sequences shorter will be padded.\n",
            "  --val_max_target_length VAL_MAX_TARGET_LENGTH\n",
            "                        The maximum total sequence length for validation\n",
            "                        target text after tokenization. Sequences longer than\n",
            "                        this will be truncated, sequences shorter will be\n",
            "                        padded. This argument is also used to override the\n",
            "                        ``max_length`` param of ``model.generate``, which is\n",
            "                        used during ``evaluate`` and ``predict``.\n",
            "  --test_max_target_length TEST_MAX_TARGET_LENGTH\n",
            "                        The maximum total sequence length for test target text\n",
            "                        after tokenization. Sequences longer than this will be\n",
            "                        truncated, sequences shorter will be padded.\n",
            "  --n_train N_TRAIN     # training examples. -1 means use all.\n",
            "  --n_val N_VAL         # validation examples. -1 means use all.\n",
            "  --n_test N_TEST       # test examples. -1 means use all.\n",
            "  --src_lang SRC_LANG   Source language id for translation.\n",
            "  --tgt_lang TGT_LANG   Target language id for translation.\n",
            "  --eval_beams EVAL_BEAMS\n",
            "                        # num_beams to use for evaluation.\n",
            "  --no_ignore_pad_token_for_loss\n",
            "                        If only pad tokens should be ignored. This assumes\n",
            "                        that `config.pad_token_id` is defined.\n",
            "  --ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]\n",
            "                        If only pad tokens should be ignored. This assumes\n",
            "                        that `config.pad_token_id` is defined.\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written.\n",
            "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
            "                        Overwrite the content of the output directory.Use this\n",
            "                        to continue training if output_dir points to a\n",
            "                        checkpoint directory.\n",
            "  --do_train [DO_TRAIN]\n",
            "                        Whether to run training.\n",
            "  --do_eval [DO_EVAL]   Whether to run eval on the dev set.\n",
            "  --do_predict [DO_PREDICT]\n",
            "                        Whether to run predictions on the test set.\n",
            "  --evaluation_strategy {no,steps,epoch}\n",
            "                        The evaluation strategy to use.\n",
            "  --prediction_loss_only [PREDICTION_LOSS_ONLY]\n",
            "                        When performing evaluation and predictions, only\n",
            "                        returns the loss.\n",
            "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for training.\n",
            "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_train_batch_size`\n",
            "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
            "                        training.\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
            "                        is preferred.Batch size per GPU/TPU core/CPU for\n",
            "                        evaluation.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass.\n",
            "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
            "                        Number of predictions steps to accumulate before\n",
            "                        moving the tensors to the CPU.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for AdamW.\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight decay for AdamW if we apply some.\n",
            "  --adam_beta1 ADAM_BETA1\n",
            "                        Beta1 for AdamW optimizer\n",
            "  --adam_beta2 ADAM_BETA2\n",
            "                        Beta2 for AdamW optimizer\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for AdamW optimizer.\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform.\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs.\n",
            "  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}\n",
            "                        The scheduler type to use.\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps.\n",
            "  --logging_dir LOGGING_DIR\n",
            "                        Tensorboard log dir.\n",
            "  --logging_first_step [LOGGING_FIRST_STEP]\n",
            "                        Log the first global_step\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps.\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps.\n",
            "  --save_total_limit SAVE_TOTAL_LIMIT\n",
            "                        Limit the total amount of checkpoints.Deletes the\n",
            "                        older checkpoints in the output_dir. Default is\n",
            "                        unlimited checkpoints\n",
            "  --no_cuda [NO_CUDA]   Do not use CUDA even when it is available\n",
            "  --seed SEED           Random seed that will be set at the beginning of\n",
            "                        training.\n",
            "  --fp16 [FP16]         Whether to use 16-bit (mixed) precision (through\n",
            "                        NVIDIA Apex) instead of 32-bit\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "                        For fp16: Apex AMP optimization level selected in\n",
            "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
            "                        https://nvidia.github.io/apex/amp.html\n",
            "  --fp16_backend {auto,amp,apex}\n",
            "                        The backend to be used for mixed precision.\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank\n",
            "  --tpu_num_cores TPU_NUM_CORES\n",
            "                        TPU: Number of TPU cores (automatically passed by\n",
            "                        launcher script)\n",
            "  --tpu_metrics_debug [TPU_METRICS_DEBUG]\n",
            "                        Deprecated, the use of `--debug` is preferred. TPU:\n",
            "                        Whether to print debug metrics\n",
            "  --debug [DEBUG]       Whether to print debug metrics on TPU\n",
            "  --dataloader_drop_last [DATALOADER_DROP_LAST]\n",
            "                        Drop the last incomplete batch if it is not divisible\n",
            "                        by the batch size.\n",
            "  --eval_steps EVAL_STEPS\n",
            "                        Run an evaluation every X steps.\n",
            "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
            "                        Number of subprocesses to use for data loading\n",
            "                        (PyTorch only). 0 means that the data will be loaded\n",
            "                        in the main process.\n",
            "  --past_index PAST_INDEX\n",
            "                        If >=0, uses the corresponding part of the output as\n",
            "                        the past state for next step.\n",
            "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
            "                        wandb logging.\n",
            "  --disable_tqdm DISABLE_TQDM\n",
            "                        Whether or not to disable the tqdm progress bars.\n",
            "  --no_remove_unused_columns\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
            "                        The list of keys in your dictionary of inputs that\n",
            "                        correspond to the labels.\n",
            "  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\n",
            "                        Whether or not to load the best model found during\n",
            "                        training at the end of training.\n",
            "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
            "                        The metric to use to compare two different models.\n",
            "  --greater_is_better GREATER_IS_BETTER\n",
            "                        Whether the `metric_for_best_model` should be\n",
            "                        maximized or not.\n",
            "  --ignore_data_skip [IGNORE_DATA_SKIP]\n",
            "                        When resuming training, whether or not to skip the\n",
            "                        first epochs and batches to get to the same training\n",
            "                        data.\n",
            "  --sharded_ddp [SHARDED_DDP]\n",
            "                        Whether or not to use sharded DDP training (in\n",
            "                        distributed training only).\n",
            "  --deepspeed DEEPSPEED\n",
            "                        Enable deepspeed and pass the path to deepspeed json\n",
            "                        config file (e.g. ds_config.json)\n",
            "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
            "                        The label smoothing epsilon to apply (zero means no\n",
            "                        label smoothing).\n",
            "  --adafactor [ADAFACTOR]\n",
            "                        whether to use adafactor\n",
            "  --group_by_length [GROUP_BY_LENGTH]\n",
            "                        Whether or not to group samples of roughly the same\n",
            "                        length together when batching.\n",
            "  --report_to REPORT_TO [REPORT_TO ...]\n",
            "                        The list of integrations to report the results and\n",
            "                        logs to.\n",
            "  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\n",
            "                        When using distributed training, the value of the flag\n",
            "                        `find_unused_parameters` passed to\n",
            "                        `DistributedDataParallel`.\n",
            "  --no_dataloader_pin_memory\n",
            "                        Whether or not to pin memory for DataLoader.\n",
            "  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\n",
            "                        Whether or not to pin memory for DataLoader.\n",
            "  --label_smoothing LABEL_SMOOTHING\n",
            "                        The label smoothing epsilon to apply (if not zero).\n",
            "  --sortish_sampler [SORTISH_SAMPLER]\n",
            "                        Whether to SortishSamler or not.\n",
            "  --predict_with_generate [PREDICT_WITH_GENERATE]\n",
            "                        Whether to use generate to calculate generative\n",
            "                        metrics (ROUGE, BLEU).\n",
            "  --encoder_layerdrop ENCODER_LAYERDROP\n",
            "                        Encoder layer dropout probability. Goes into\n",
            "                        model.config.\n",
            "  --decoder_layerdrop DECODER_LAYERDROP\n",
            "                        Decoder layer dropout probability. Goes into\n",
            "                        model.config.\n",
            "  --dropout DROPOUT     Dropout probability. Goes into model.config.\n",
            "  --attention_dropout ATTENTION_DROPOUT\n",
            "                        Attention dropout probability. Goes into model.config.\n",
            "  --lr_scheduler LR_SCHEDULER\n",
            "                        Which lr scheduler to use. Selected in ['constant',\n",
            "                        'constant_w_warmup', 'cosine', 'cosine_w_restarts',\n",
            "                        'linear', 'polynomial']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DDXpoxZrZA9M",
        "outputId": "d166edf8-6b64-4b08-b99b-1ed7ee07bf0f"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/211130_test/finetune-t5-ynat-code'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8DluZxVYDTc",
        "outputId": "e359ca7f-724d-4564-a45c-2abffe283387"
      },
      "source": [
        "# 밑에 wandb 코드 먼저 돌리고 이거 돌리시면 train, eval까지 됩니다! (약 5시간+ 소요)\n",
        "!CUDA_VISIBLE_DEVICES=1 python seq2seq_finetune_t5_ynat.py \\\n",
        "--do_train --do_eval --predict_with_generate \\\n",
        "--model_name_or_path /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5 \\ # 에트리 모델이 있는 경로 (이건 그대로 쓰시면 됩니다)\n",
        "--data_dir /content/drive/MyDrive/ET5_test/ynat-v1.1 \\ #(위에서 다운받은 ynat데이터가 위치하는 경로 - 데이터 디렉토리)\n",
        "--output_dir /content/drive/MyDrive/ET5_test/output \\ #(아웃풋을 저장할 경로: 저는 미리 아웃풋 폴더를 만들어놓고 거기로 경로 설정하였습니다)\n",
        "--overwrite_output_dir \\\n",
        "--save_steps 100000 \\\n",
        "--per_device_train_batch_size 16 \\\n",
        "--gradient_accumulation_steps 1 \\\n",
        "--num_train_epochs 1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/02/2021 06:06:19 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "12/02/2021 06:06:19 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/ET5_test/output', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Dec02_06-06-19_56547a973875', logging_first_step=False, logging_steps=500, save_steps=100000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/ET5_test/output', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard', 'wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, label_smoothing=0.0, sortish_sampler=False, predict_with_generate=True, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n",
            "[INFO|configuration_utils.py:447] 2021-12-02 06:06:19,918 >> loading configuration file /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5/config.json\n",
            "[INFO|configuration_utils.py:485] 2021-12-02 06:06:19,919 >> Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 45100\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:447] 2021-12-02 06:06:19,921 >> loading configuration file /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5/config.json\n",
            "[INFO|configuration_utils.py:485] 2021-12-02 06:06:19,921 >> Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 45100\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1688] 2021-12-02 06:06:19,921 >> Model name '/content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5' not found in model shortcut name list (t5-small, t5-base, t5-large, t5-3b, t5-11b). Assuming '/content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "[INFO|tokenization_utils_base.py:1721] 2021-12-02 06:06:19,923 >> Didn't find file /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5/tokenizer.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1721] 2021-12-02 06:06:19,925 >> Didn't find file /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1721] 2021-12-02 06:06:19,925 >> Didn't find file /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5/special_tokens_map.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1784] 2021-12-02 06:06:19,927 >> loading file /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1784] 2021-12-02 06:06:19,927 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1784] 2021-12-02 06:06:19,927 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1784] 2021-12-02 06:06:19,927 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1784] 2021-12-02 06:06:19,927 >> loading file /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:1025] 2021-12-02 06:06:20,145 >> loading weights file /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1143] 2021-12-02 06:06:30,920 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-12-02 06:06:30,920 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "#####\t Reading an input file ...\t /content/drive/MyDrive/ET5_test/ynat-v1.1/train.json\n",
            "#####\t Create examples ... : 45678it [00:00, 505154.49it/s]\n",
            "#####\t Get source and target texts ... : 100% 45677/45677 [00:00<00:00, 1154108.85it/s]\n",
            "#####\t Reading an input file ...\t /content/drive/MyDrive/ET5_test/ynat-v1.1/val.json\n",
            "#####\t Create examples ... : 9107it [00:00, 542493.74it/s]\n",
            "#####\t Get source and target texts ... : 100% 9106/9106 [00:00<00:00, 1444583.09it/s]\n",
            "12/02/2021 06:06:33 - INFO - __main__ -   *** Train ***\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:705: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:724] 2021-12-02 06:06:33,316 >> Loading model from /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5).\n",
            "[INFO|configuration_utils.py:447] 2021-12-02 06:06:33,318 >> loading configuration file /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5/config.json\n",
            "[INFO|configuration_utils.py:485] 2021-12-02 06:06:33,319 >> Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.3.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 45100\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1025] 2021-12-02 06:06:33,321 >> loading weights file /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1143] 2021-12-02 06:06:43,822 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-12-02 06:06:43,823 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/cakd3_3차프로젝트_2조/Datasets/ETRI_ET5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "[INFO|trainer.py:837] 2021-12-02 06:06:43,834 >> ***** Running training *****\n",
            "[INFO|trainer.py:838] 2021-12-02 06:06:43,834 >>   Num examples = 45676\n",
            "[INFO|trainer.py:839] 2021-12-02 06:06:43,834 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:840] 2021-12-02 06:06:43,834 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:841] 2021-12-02 06:06:43,834 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:842] 2021-12-02 06:06:43,834 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:843] 2021-12-02 06:06:43,834 >>   Total optimization steps = 2855\n",
            "[INFO|integrations.py:546] 2021-12-02 06:06:43,880 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseuly1203\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/drive/MyDrive/ET5_test/output\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/seuly1203/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/seuly1203/huggingface/runs/28w8ibvz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/drive/My Drive/211130_test/finetune-t5-ynat-code/wandb/run-20211202_060644-28w8ibvz\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "  0% 1/2855 [00:06<5:20:26,  6.74s/it]y\n",
            "{'loss': 0.6321, 'learning_rate': 4.124343257443082e-05, 'epoch': 0.18}\n",
            " 35% 1000/2855 [1:36:56<4:14:26,  8.23s/it]{'loss': 0.1821, 'learning_rate': 3.248686514886165e-05, 'epoch': 0.35}\n",
            "{'loss': 0.1722, 'learning_rate': 2.3730297723292473e-05, 'epoch': 0.53}\n",
            " 70% 2000/2855 [3:13:20<1:53:15,  7.95s/it]{'loss': 0.1611, 'learning_rate': 1.4973730297723293e-05, 'epoch': 0.7}\n",
            " 88% 2500/2855 [4:00:55<45:39,  7.72s/it]{'loss': 0.1482, 'learning_rate': 6.2171628721541155e-06, 'epoch': 0.88}\n",
            "100% 2855/2855 [4:34:38<00:00,  5.34s/it][INFO|trainer.py:1007] 2021-12-02 10:41:26,921 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 16483.0865, 'train_samples_per_second': 0.173, 'epoch': 1.0}\n",
            "100% 2855/2855 [4:34:38<00:00,  5.77s/it]\n",
            "[INFO|trainer.py:1408] 2021-12-02 10:41:26,929 >> Saving model checkpoint to /content/drive/MyDrive/ET5_test/output\n",
            "[INFO|configuration_utils.py:304] 2021-12-02 10:41:26,938 >> Configuration saved in /content/drive/MyDrive/ET5_test/output/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-12-02 10:41:31,381 >> Model weights saved in /content/drive/MyDrive/ET5_test/output/pytorch_model.bin\n",
            "12/02/2021 10:41:33 - INFO - __main__ -   ***** train metrics *****\n",
            "12/02/2021 10:41:33 - INFO - __main__ -     epoch = 1.0\n",
            "12/02/2021 10:41:33 - INFO - __main__ -     train_n_objs = -1\n",
            "12/02/2021 10:41:33 - INFO - __main__ -     train_runtime = 16483.0865\n",
            "12/02/2021 10:41:33 - INFO - __main__ -     train_samples_per_second = 0.173\n",
            "12/02/2021 10:41:33 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:1600] 2021-12-02 10:41:33,184 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1601] 2021-12-02 10:41:33,184 >>   Num examples = 9105\n",
            "[INFO|trainer.py:1602] 2021-12-02 10:41:33,184 >>   Batch size = 8\n",
            "100% 1139/1139 [31:02<00:00,  1.20s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 2855 < 2856; dropping {'train/val_loss': 0.15993009507656097, 'train/val_F1(macro)': 0.8704296950566635, 'train/val_runtime': 1865.0124, 'train/val_samples_per_second': 4.882, 'train/epoch': 1.0}.\n",
            "100% 1139/1139 [31:03<00:00,  1.64s/it]\n",
            "12/02/2021 11:12:38 - INFO - __main__ -   ***** val metrics *****\n",
            "12/02/2021 11:12:38 - INFO - __main__ -     epoch = 1.0\n",
            "12/02/2021 11:12:38 - INFO - __main__ -     val_F1(macro) = 0.8704296950566635\n",
            "12/02/2021 11:12:38 - INFO - __main__ -     val_loss = 0.1599\n",
            "12/02/2021 11:12:38 - INFO - __main__ -     val_n_objs = -1\n",
            "12/02/2021 11:12:38 - INFO - __main__ -     val_runtime = 1865.0124\n",
            "12/02/2021 11:12:38 - INFO - __main__ -     val_samples_per_second = 4.882\n",
            "12/02/2021 11:12:38 - INFO - __main__ -   {'train_runtime': 16483.0865, 'train_samples_per_second': 0.173, 'epoch': 1.0, 'train_n_objs': -1, 'val_loss': 0.1599, 'val_F1(macro)': 0.8704296950566635, 'val_runtime': 1865.0124, 'val_samples_per_second': 4.882, 'val_n_objs': -1}\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 694... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/epoch ▁▂▄▅▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/learning_rate █▆▅▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/loss █▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/epoch 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/learning_rate 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/loss 0.1482\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/total_flos 2666446777700352\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/train_runtime 16483.0865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_samples_per_second 0.173\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/content/drive/MyDrive/ET5_test/output\u001b[0m: \u001b[34mhttps://wandb.ai/seuly1203/huggingface/runs/28w8ibvz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211202_060644-28w8ibvz/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zBNb6d0TnjBw",
        "outputId": "c1b20958-fb9f-40b8-f295-564f4fe0fe9c"
      },
      "source": [
        "# 이코드 먼저 돌리고 위에 돌리세요\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.8.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.15.7)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.0-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.25.1)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.1.0-py3-none-any.whl (19 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6488 sha256=63833b203163384b2d1b944d6af705cd6e0e5874d0b2e858f8907235c43ab72c\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=e0c4618770a358b2fe606933aa816f8368bcff01b3632107aee88dec5419131e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, docker-pycreds, configparser, wandb\n",
            "Successfully installed configparser-5.1.0 docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-1.5.0 shortuuid-1.0.8 subprocess32-3.5.4 wandb-0.12.7 yaspin-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/seuly1203/uncategorized/runs/oa6mc1c3\" target=\"_blank\">dutiful-darkness-2</a></strong> to <a href=\"https://wandb.ai/seuly1203/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f02312295d0>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/seuly1203/uncategorized/runs/oa6mc1c3?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCoY71nE9fim"
      },
      "source": [
        "- Traceback (most recent call last):\n",
        "  - File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 424, in get_config_dict\n",
        "    use_auth_token=use_auth_token,\n",
        "  - File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 1086, in cached_path\n",
        "    local_files_only=local_files_only,\n",
        "  - File \"/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\", line 1216, in get_from_cache\n",
        "    r.raise_for_status()\n",
        "  - File \"/usr/local/lib/python3.7/dist-packages/requests/models.py\", line 943, in raise_for_status\n",
        "    raise HTTPError(http_error_msg, response=self)\n",
        "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/PRETRAINED_MODEL_DIR/resolve/main/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Sesjo282D8"
      },
      "source": [
        "- line 379, in <module>\n",
        "    main()\n",
        "- line 199, in main\n",
        "    cache_dir=model_args.cache_dir\n",
        "/content/drive/MyDrive/211130_test/finetune-t5-ynat-code/seq2seq_finetune_t5_ynat.py\n",
        "- line 368, in from_pretrained\n",
        "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
        "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/configuration_auto.py\n",
        "- line 436, in get_config_dict\n",
        "    raise EnvironmentError(msg)\n",
        "/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj3pQtBCDQ-5",
        "outputId": "1920926a-c0ab-45cd-fa06-e4e2d2bdef9f"
      },
      "source": [
        "#!nvcc --version #cuda버전확인하는 코드"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I19WGStIyPE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}